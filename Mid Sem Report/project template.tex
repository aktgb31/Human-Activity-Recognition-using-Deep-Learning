\documentclass[twocolumn]{article}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multicol}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
   \date{}
\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace{1cm}
        \normalsize
        \textbf{Project Report on}\\
        \vspace{0.5cm}

        \Large
        \textbf{Human Activity Recognition from video sequences using Deep Learning}\\
        \vspace{0.5cm}
        \emph{Submitted by}\\
        \vspace{0.5cm}
        \large
        \textbf{Uttkarsh Raj} \hspace{0.75cm}
        \textbf{B190955CS}\\
        \textbf{Amit Kumar} \hspace{0.75cm}
        \textbf{B190343CS}\\
        \textbf{Moturu Manogna} \hspace{0.75cm}
        \textbf{B190695CS}\\
        %\textbf{Group Members:}\\
        %\vfill
        \vspace{0.2cm}
        \emph{Under the Guidance of}\\
        \large
        \vspace{0.5cm}
        \textbf{Pournami PN}

        \vspace{.5cm}
        \begin{center}
            \includegraphics[width=0.4\textwidth]{nitc-logo.png}
        \end{center}
        \vspace{0.8cm}
        \textbf{Department of Computer Science and Engineering}\\
        \textbf{National Institute of Technology Calicut}\\
        \textbf{Calicut, Kerala, India - 673 601}\\
        \vspace{0.8cm}
        \textbf{October 11, 2022} %Enter the date
    \end{center}
\end{titlepage}

\title{\textbf{Human Activity Recognition from video sequences using Deep Learning}}\\
\author{Uttkarsh Raj \and Amit Kumar \and Moturu Manogna}

\twocolumn[
    \begin{@twocolumnfalse}
        \maketitle
        \textbf{
            \textit\textbf{{Abstract:}}
            AI is replacing humans in various laborious tasks, including watching video
            surveillance streams to detect unusual actions at airports, railway stations, bus stops, and other
            public gatherings, summarising human actions in a video, etc. A human doing these activities
            leaves a scope of error due to negligence and is cost-ineffective. Our project is to identify human activities
            and the time in the video at which that activity took place using deep learning and video data processing methods.
            Unlike image processing, video processing requires many input parameters and computational power to train the
            model. Our model takes a video clip as input and outputs the name and time of the activity in the video.
        }\vspace{0.5cm}
    \end{@twocolumnfalse}
]


\section{Introduction}
Human activity recognition is the task of identifying activities done by a human in a live video stream, recorded video clip, or sequence of images. For example, walking,
Running, dancing, playing cricket, jumping, etc. The two goals involved in a HAR system are to identify the activity and the time of activity in video. This data is
further used to trigger some actions. HAR systems can be used along with video surveillance cameras to enhance security and well-being by identifying suspicious
activities, crimes, and accidents in public places like airports, bus stands, forests, mountains, and other remote areas. There are many applications of the HAR
system in healthcare, like monitoring patient activities, developing human-computer interfaces like giving commands to computers through hand actions, virtual reality,
and military uses like identifying terrorist activities, etc. An alert is generated in HAR systems on certain human activities, which is further sent to the control room
for further inspection or trigger some actions. HAR systems reduce the scope of error due to human negligence in surveillance systems, reduce the cost of monitoring
and deployment, and can be easily deployed to cover large and remote areas. \\
Types of HAR Systems:
On the basis of equipment, there are two main categories of HAR (Human Activity Recognition) systems :
Vision-Based Human Activity Recognition and Sensor-Based Human Activity Recognition.\\
Vision-Based Human Activity Recognition:
Vision-Based HAR is the task of capturing the video by installing static cameras at various places for observation purposes and sending it to the servers.
These camera security footages or camera-recorded clips are used to keep an eye and predict the movement of humans using that recording. We can use this type of HAR for
security, the medical field, visual monitoring, irregular behavior detection, road safety, crowd monitoring, etc. In vision-based HAR, only camera recordings are used, no
sensors are used for action recognition.\\
Sensor-Based Human Activity Recognition:
Sensor-Based HAR is a technology that can recognize human activities through sensors. In this method, data is fetched from the sensors, which can either be present in
smartphones or any wearable device. In vision-based HAR, cameras are installed at fixed positions, so action recognition is limited. In sensor-based HAR, data received
from sensors is for a specific task, but in the case of cameras, it contains data
from another non-target human in the view angle. For sensors, there is no limitation on the position of sensors.

\section{Problem statement}
To design and develop a vision-based human activity recognition system to identify human activities and the time of the activity in a video. The input to this system
will be a video clip. The output will be the name of the activity and its time of occurrence.

\section{Literature Review}
\cite{b1} This paper discusses video processing using deep learning techniques. It discusses the applications of video processing in real life like entertainment, surveillance, crowd
management etc, functionalities of video processing in computer vision context like Human Action Recognition (HAR), motion detection, object detection, object recognition, object tracking,
video classification, behavior analysis, background subtraction, event recognition, action segmentation and scene understanding. The paper further talks about the techniques used for video processing,
data sets generally used to train the model and the challenges faced like poor quality of videos, complexity in tracking and locating multisubject, dynamic backgrounds, and lack of open research
datasets and computation power.\\
\cite{b2} This paper discusses the problems faced in Human Activity Recognition and the research which have been done on video processing.
A review of different types of video-based HAR methods, i) HandCrafted Feature-Based Approach, ii) Deep Learning Approach, and various benchmark video datasets are given.
Deep learning methods like CNN (Convolutional Neural Network) approach, RNN (Recurrent Neural Network), and LSTM (Long Short Term Memory) with CNN are reviewed with all the details
about the study done on these topics. The paper expresses low-quality videos, lack of dataset, complex and dynamic background and activities, and design constraints of real-time HAR video systems
as major challenges.\\
\cite{b3} This paper discusses deep learning techniques used for video-based human action recognition. The paper discusses current state-of-the art in human action recognition
systems built using Convolution Neural Networks (CNNs), Recurrent Neural Networks - Long Short Term Memory (RNN-LSTMs), Deep Belief Networks (DBNs), and Stacked Denoising Autoencoders(SDAs).
The paper also discusses the future research directions in the field of human action recognition, like developing unsupervised learning models, as the cost of labeling data is very high
in terms of money and manpower, developing deeper CNNs, combining different learning models in a single framework, fusion of hand-crafted and deep learning solutions, and using transfer learning.

\subsection{Vision-based HAR Methods}
Researchers have presented many handcrafted feature-based and deep learning-based approaches over the decade. However, deep learning-based HAR techniques are used over
the handcrafted-feature-based approach because in the latter, the commonly used extractors are developed based on a specific dataset, and the extractors are database-biased, general
purpose feature extraction ability is absent, and it is a labor-intensive and time-consuming technique.\\
CNNs are one of the most popular neural deep learning models used to process visual data and are used for image processing. One significant benefit of CNNs
is that they can operate directly on raw data without requiring any hand-crafted feature extraction. A video can be divided into a sequence of images, and CNN
can be applied to each of the images. The two-stream convolutional network proposed by Sismonyan and Zisserman \cite{b4} has shown strong performance
for human action recognition in videos. This model is a two-stream architecture including the spatial stream and the temporal stream, where each stream is executed by a
CNN. The first stream recognizes actions from a single frame, while the second recognizes actions from motion information of multi-frame optical flow. These two streams
are then combined for the classification task. It showed a very good performance with limited training data. However, the two-stream architecture is not applicable for human
activity recognition in live video cameras due to higher computational complexity.\\
However, video classification is more than just a simple image classification. To model complex dynamics of different actions, a Recurrent Neural Network (RNN) with long short-term memory(LSTM)
is used because RNN allows us to access the long-range information of a temporal sequence.
The authors proposed in \cite{b5} a novel technique that combines CNN and deep bidirectional LSTM network(DB-LSTM). Deep features are extracted from every sixth frame of the
videos. Then, sequential information is learned from the frames using DB-LSTM. This model is capable of learning long-term sequences and can process lengthy videos
by analyzing features for a certain time interval.

\subsection{Datasets}
We have identified some of the benchmark HAR datasets that will be used to train and test the system. These datasets are published for the general public use by famous
institutions.
\begin{itemize}
    \item \cite{b6} UFC101 dataset contains 132320 realistic action videos taken from youtube that are divided into 101 categories with 100-200 videos in each category.
    \item \cite{b7} HMDB51 dataset contains 6849 action video clips divided into 51 classes, each containing more than 100 clips.
    \item \cite{b8} Kinetics 400 dataset contains 240k videos containing 400 human action classes and with more than 400 clips per class.
\end{itemize}

\section{Work Done}
\begin{itemize}
    \item We identified the problem domain, formulated the problem statement, and mentioned the input and output specifications.
    \item We have gone through machine learning and convolutional neural networks courses on Coursera and read articles on the same.
    \item We have done a literature survey on commonly used techniques used in human activity recognition systems.
    \item We have identified some of the useful datasets that will be used for training and testing the system.
\end{itemize}

\section{Work Plan}
\begin{itemize}
    \item We will explore more deep-learning techniques for human activity recognition, analyze their pros and cons, and choose a suitable method as a base for this system.
    \item We will further identify more datasets large enough to avoid overfitting, cover a large class of actions and be processable within time constraints.
    \item We will create a base design of the system.
\end{itemize}

\section{Conclusion}
We have successfully identified the problem domain, formulated the problem statement, and mentioned the input and output. We have also identified some useful
deep-learning techniques and looked through a few datasets. We intend to further improve our knowledge of more deep-learning techniques, look for more datasets and
create a base design of the system by the end of this semester.

% Last section is References. A sample bibliography is given as example. 

% \section*{References}
%
%Please number citations consecutively within brackets \cite{b1}. The 
%sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
%number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
%the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''
%
%Number footnotes separately in superscripts. Place the actual footnote at 
%the bottom of the column in which it was cited. Do not put footnotes in the 
%abstract or reference list. Use letters for table footnotes.
%
%Unless there are six authors or more give all authors' names; do not use 
%``et al.''. Papers that have not been published, even if they have been 
%submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
%that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
%Capitalize only the first word in a paper title, except for proper nouns and 
%element symbols.
%
%For papers published in translation journals, please give the English 
%citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
    \bibitem{b1}: Vijeta Sharma, Manjari Gupta, Ajai Kumari, and Deepti Mishra (2021) Video Processing Using Deep Learning Techniques: A Systematic Literature Review
    \bibitem{b2}: Vijeta Sharma, Manjari Gupta, Anil Kumar Pandey, Deepti Mishra, and Ajai Kumar (2022) A Review of Deep Learning-based Human Activity Recognition on Benchmark Video Datasets
    \bibitem{b3}: Hieu H. Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, and Sergio A. Velastin (2022) Video-based Human Action Recognition using Deep Learning: A Review
    \bibitem{b4}: K. Simonyan and A. Zisserman, "Two-stream convolutional networks for action recognition in videos," in Advances in Neural Information Processing Systems, 2014
    \bibitem{b5}: Amin Ullah, Jamil Ahmad, Khan Muhammad, Muhammad Sajjad, Sung Wook Baik,(2018) Action Recognition in Video Sequences using Deep Bi-Directional LSTM With CNN Features
    \bibitem{b6}: Soomro, K., A. Roshan Zamir, and M. Shah. 2012. "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild". November. http://arxiv.org/abs/1212.0402 .
    \bibitem{b7}: Kuehne, H., H. Jhuang, E. Garrote, T. Poggio, and T. Serre. 2011. HMDB: A Large Video Database for Human Motion Recognition, 2011 International Conference on Computer Vision, 2011, pp. 2556-2563, DOI: 10.1109/ICCV.2011.6126543
    \bibitem{b8}: Kay, W., J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, and F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman . 2017. The Kinetics Human Action Video Dataset. ArXiv
\end{thebibliography}
%\vspace{12pt}
%%\color{red}
%IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
